{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8843fbd",
   "metadata": {},
   "source": [
    "# Bias / Variance / Irreducible Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed47489",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "This part of the generalisation error is due to wrong assumptions, such as assuming that data is linear when it is actually quandratic. A high-bias model is most like to underfit the training data. (High MSE in training data)\n",
    "\n",
    "\n",
    "## Variance\n",
    "\n",
    "This part is due to the model's excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree polynomial model) is likely to have high variance, and thus to overfit the training data. (Low MSE in Training Data, High MSE in Validation/Test Data)\n",
    "\n",
    "\n",
    "\n",
    "## Sweet Spot\n",
    "The goal is optimise the sweet spot with reasonable bias/variance, preferablly low bias, low variance. However, a low bias typically results in high varance (overfit) and viceversa (underfit).\n",
    "\n",
    "The ways in optimising are:\n",
    "- Regularisation - to reduce overfit by adding a paramer against weights\n",
    "- Boosting (Emsemble)\n",
    "- Bagging (random forest)\n",
    "\n",
    "\n",
    "## Irreducible Error\n",
    "This part is due to the noisiness of the data itself. The only way to reduce this part of the error is to clean up data.\n",
    "\n",
    "Bias and Variance is always a trade off between model complexity (more complex less bias more variance.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b248c12-501a-4439-958a-7cc61a0a22e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ecb3bfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Regularisation\n",
    "\n",
    "There is a good article on this via sklearn: https://scikit-learn.org/stable/modules/linear_model.html\n",
    "\n",
    "**Note** \n",
    "\n",
    "- Regularisation term is only added to cost function during training.\n",
    "- Y-intercept is NOT included\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee45dc-5324-41d5-8eaf-e5870cd3e549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "439bf0d1-8d26-4eff-a6e8-155c0bca7097",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "Ridge Regrssion (Tikhnov regularisation) is a regularised version of linear regression. Forces model not only fit but keep the weights as small as possible.\n",
    "\n",
    "Sum of all weight^2 with a hyper parameter (lambda which controlls how much regularisation is applied. if 0 then typical linear regression). Higher the lambda the flatter the slope, therefore reducing variance increasiong bias. The slope asymptote to zero\n",
    "\n",
    "## Cost function\n",
    "**note** i start from 1 for the regularisation term\n",
    "\n",
    "$j(\\theta) = MSE(\\theta) + \\frac{\\lambda\\sum_{i=1}^n(\\theta^2)}{2}$\n",
    "\n",
    "where:\n",
    "\n",
    "$sum(\\theta^2) = l2 norm$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b6559de-38b2-4de5-b248-c2c281569cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "X, y, coef = make_regression(coef = True, noise = 10, bias = 5)\n",
    "\n",
    "ridge_reg = Ridge(alpha = 1, solver = \"cholesky\") # note sometimes lambda is referred as alpha. \n",
    "ridge_reg.fit(X, y)\n",
    "\n",
    "theta_1 = ridge_reg.coef_\n",
    "bias_1 = ridge_reg.intercept_\n",
    "\n",
    "ridge_reg = Ridge(alpha = 4, solver = \"cholesky\") # note sometimes lambda is referred as alpha. \n",
    "\n",
    "ridge_reg.fit(X, y)\n",
    "\n",
    "theta_2 = ridge_reg.coef_\n",
    "bias_2 = ridge_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3ed236e-e773-4dfd-a16c-01cb89e99109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.944275448163377 3.4133960363438054\n",
      "5.4007930428875 2.4395440048724204\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.mean(theta_1), np.mean(theta_2))\n",
    "print(bias_1, bias_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ff895",
   "metadata": {},
   "source": [
    "## Lasso Regrssion\n",
    "Least absolute shrinkage and selection operator regression, using l1 norm instead of l2 norm\n",
    "\n",
    "### Cost function\n",
    "$j(\\theta) = MSE(\\theta) + \\lambda\\sum(abs(\\theta))$\n",
    "\n",
    "\n",
    "### Elastic Net\n",
    "j(theta) = MSE(theta) + r*alpha* sum(abs(theta)) + (1-r)/2 * alpha * sum(theta^2)\n",
    "\n",
    "Note that ridge regression is a useful technique if as it automatically reduce features, however if there are features of importance it would be better to use Lasso or Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de44099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
