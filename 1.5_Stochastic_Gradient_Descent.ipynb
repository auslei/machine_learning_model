{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21d64ba",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "### Batch Gradient Descent\n",
    "Compute gradients for the whole batch, adjusting theta by minus it against the partial derivative of it with a leanring rate multiplier. The higher the rate the fast, but may result in overshooting. The lower the rate the slower, but may stuck.\n",
    "\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "Picks a random instance of the training set at every step and compute gradients at the step. This is fast due to size for each step, and makes it possible for training large datasets.  However, less regular due to the random nature and cost may go up and down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd1fc559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE cost function (root mean square error)\n",
    "# typically used as error function for regression models\n",
    "# the goal is to minimize the error function by finding tbe best\n",
    "# thetas\n",
    "def MSE(X, Theta, y):\n",
    "    y_hat = X.dot(Theta)\n",
    "    MSE = np.sum(np.square(y_hat - y))\n",
    "    return MSE\n",
    "\n",
    "# Vectorised version of partial differential equation \n",
    "def d_MSE(X_b, theta, y):    \n",
    "    return 2 / len(X) * X_b.T.dot(X_b.dot(theta) - y)\n",
    "\n",
    "\n",
    "# Flexible learning rate, decrease over t\n",
    "def learning_schedule(t, t0 = 5, t1 = 50):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "# stochastic Gradient Descent\n",
    "# initial hyper parameter to adjust to ensure no zero division and reduce learning rate when epoch is higher\n",
    "def SGD(n_epochs, X, theta, y, t0 = 5, t1 = 50):   \n",
    "    m = len(X)\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(m):        \n",
    "            random_index = np.random.randint(m)\n",
    "            xi = X[random_index:random_index + 1]\n",
    "            yi = y[random_index:random_index + 1]\n",
    "            \n",
    "            gradients = d_MSE(xi, theta, yi)\n",
    "            \n",
    "            learning_rate = learning_schedule(epoch + i)\n",
    "            theta = theta - learning_rate * gradients                        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"epoch {epoch}, {MSE(X, theta, y)}, {learning_rate}\")                \n",
    "    return theta\n",
    "\n",
    "# mini batch gradient descent\n",
    "# same as stocastic, but mini random batches of a time\n",
    "def mini_batch_SGD(n_epochs, X, theta, y, batch_size = 1, t0 = 5, t1 = 50):   \n",
    "    m = len(X)\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(m):        \n",
    "            random_index = np.random.randint(m)\n",
    "            xi = X[random_index:random_index + batch_size]\n",
    "            yi = y[random_index:random_index + batch_size]\n",
    "            \n",
    "            gradients = d_MSE(xi, theta, yi)\n",
    "            \n",
    "            learning_rate = t0 / (epoch * m + i + t1)\n",
    "            theta = theta - learning_rate * gradients                                          \n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6753298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, 23588.665477461363, 0.020080321285140562\n",
      "epoch 100, 269.65263308778145, 0.014326647564469915\n",
      "epoch 200, 134.26578365722804, 0.011135857461024499\n",
      "epoch 300, 96.63412773069106, 0.009107468123861567\n",
      "epoch 400, 82.7996389582145, 0.007704160246533128\n",
      "epoch 500, 76.35809917559075, 0.006675567423230975\n",
      "epoch 600, 72.94899607942767, 0.005889281507656066\n",
      "epoch 700, 71.03906600194541, 0.005268703898840885\n",
      "epoch 800, 69.90909947928242, 0.004766444232602479\n",
      "epoch 900, 69.21041203430285, 0.004351610095735422\n",
      "[[10.77301113]\n",
      " [ 3.18765278]]\n",
      "[[10.79155133]\n",
      " [ 3.19731374]]\n"
     ]
    }
   ],
   "source": [
    "###import librarries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "### - Generate random dataset with default size = 100 and intercept at around 5\n",
    "### - Adding some noices around the line. \n",
    "### - Note the slope and intercept (As described above)\n",
    "def generate_random_data(size = 100, intercept = 5, noise = True):\n",
    "    X = 2 * np.random.rand(size, 1)\n",
    "    y = 3 * X + intercept\n",
    "    if noise:\n",
    "      y = y + np.random.rand(size, 1) * intercept / 5\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_random_data(size = 200, intercept = 10, noise = True)\n",
    "\n",
    "X_with_bias = np.c_[np.ones((len(X), 1)), X]\n",
    "theta = np.random.rand(2, 1) \n",
    "\n",
    "print(SGD(1000, X_with_bias, theta, y))\n",
    "print(mini_batch_SGD(100, X_with_bias, theta, y, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240acfae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
